{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML ASSIGNMENT 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newsgroups data:\n",
    "\n",
    "The data contains approximately 20,000 across 20 online newsgroups. A newsgroup is a place on the Internet where you can ask and answer questions about a certain topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "groups = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/home/sreeganesh/scikit_learn_data/20news_home/20news-bydate-train/rec.autos/102994',\n",
       "       '/home/sreeganesh/scikit_learn_data/20news_home/20news-bydate-train/comp.sys.mac.hardware/51861',\n",
       "       '/home/sreeganesh/scikit_learn_data/20news_home/20news-bydate-train/comp.sys.mac.hardware/51879',\n",
       "       ...,\n",
       "       '/home/sreeganesh/scikit_learn_data/20news_home/20news-bydate-train/comp.sys.ibm.pc.hardware/60695',\n",
       "       '/home/sreeganesh/scikit_learn_data/20news_home/20news-bydate-train/comp.graphics/38319',\n",
       "       '/home/sreeganesh/scikit_learn_data/20news_home/20news-bydate-train/rec.motorcycles/104440'],\n",
       "      dtype='|S97')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.keys()\n",
    "groups.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 18846\n",
      "Number of diffrent categories: 20\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = fetch_20newsgroups(subset='all')\n",
    "print(\"Number of articles: \" + str(len(news.data)))\n",
    "print(\"Number of diffrent categories: \" + str(len(news.target_names)))\n",
    "\n",
    "print news.target_names\n",
    "\n",
    "import numpy as np\n",
    "np.unique(groups.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### range from 0 to 19, representing 20 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitter and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sreeganesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First file: \n",
      "\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Second file: \n",
      "\n",
      "From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\n",
      "Subject: Re: Sigma Designs Double up??\n",
      "Article-I.D.: ux1.C52u8x.B62\n",
      "Organization: University of Illinois at Urbana\n",
      "Lines: 29\n",
      "\n",
      "jap10@po.CWRU.Edu (Joseph A. Pellettiere) writes:\n",
      "\n",
      "\n",
      ">\tI am looking for any information about the Sigma Designs\n",
      ">\tdouble up board.  All I can figure out is that it is a\n",
      ">\thardware compression board that works with AutoDoubler, but\n",
      ">\tI am not sure about this.  Also how much would one cost?\n",
      "\n",
      "I've had the board for over a year, and it does work with Diskdoubler,\n",
      "but not with Autodoubler, due to a licensing problem with Stac Technologies,\n",
      "the owners of the board's compression technology. (I'm writing this\n",
      "from memory; I've lost the reference. Please correct me if I'm wrong.)\n",
      "\n",
      "Using the board, I've had problems with file icons being lost, but it's\n",
      "hard to say whether it's the board's fault or something else; however,\n",
      "if I decompress the troubled file and recompress it without the board,\n",
      "the icon usually reappears. Because of the above mentioned licensing\n",
      "problem, the freeware expansion utility DD Expand will not decompress\n",
      "a board-compressed file unless you have the board installed.\n",
      "\n",
      "Since Stac has its own product now, it seems unlikely that the holes\n",
      "in Autodoubler/Diskdoubler related to the board will be fixed.\n",
      "Which is sad, and makes me very reluctant to buy Stac's product since\n",
      "they're being so stinky. (But hey, that's competition.)\n",
      "-- \n",
      "\n",
      "Stan Kerr    \n",
      "Computing & Communications Services Office, U of Illinois/Urbana\n",
      "Phone: 217-333-5217  Email: stankerr@uiuc.edu   \n",
      "\n",
      "\n",
      "\n",
      "Third file: \n",
      "\n",
      "From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\n",
      "Subject: PB questions...\n",
      "Organization: Purdue University Engineering Computer Network\n",
      "Distribution: usa\n",
      "Lines: 36\n",
      "\n",
      "well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n",
      "---------------------------------------------------------------------------\n",
      "\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\n",
      "Nietzsche\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text1 = groups.data[0] \n",
    "print \"First file: \\n\"\n",
    "print text1\n",
    "print \"\\n\"\n",
    "\n",
    "text2 = groups.data[9]\n",
    "print \"Second file: \\n\"\n",
    "print text2\n",
    "print \"\\n\"\n",
    "\n",
    "text3 = groups.data[2]\n",
    "print \"Third file: \\n\"\n",
    "print text3\n",
    "print\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\", u'Nntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day.', u'It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s.', u'It was called a Bricklin.', u'The doors were really small.', u'In addition,\\nthe front bumper was separate from the rest of the body.', u'This is \\nall I know.', u'If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.', u'Thanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----'] \n",
      "\n",
      "[u'From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubject: Re: Sigma Designs Double up??', u'Article-I.D.', u': ux1.C52u8x.B62\\nOrganization: University of Illinois at Urbana\\nLines: 29\\n\\njap10@po.CWRU.Edu (Joseph A. Pellettiere) writes:\\n\\n\\n>\\tI am looking for any information about the Sigma Designs\\n>\\tdouble up board.', u'All I can figure out is that it is a\\n>\\thardware compression board that works with AutoDoubler, but\\n>\\tI am not sure about this.', u'Also how much would one cost?', u\"I've had the board for over a year, and it does work with Diskdoubler,\\nbut not with Autodoubler, due to a licensing problem with Stac Technologies,\\nthe owners of the board's compression technology.\", u\"(I'm writing this\\nfrom memory; I've lost the reference.\", u\"Please correct me if I'm wrong.)\", u\"Using the board, I've had problems with file icons being lost, but it's\\nhard to say whether it's the board's fault or something else; however,\\nif I decompress the troubled file and recompress it without the board,\\nthe icon usually reappears.\", u'Because of the above mentioned licensing\\nproblem, the freeware expansion utility DD Expand will not decompress\\na board-compressed file unless you have the board installed.', u'Since Stac has its own product now, it seems unlikely that the holes\\nin Autodoubler/Diskdoubler related to the board will be fixed.', u\"Which is sad, and makes me very reluctant to buy Stac's product since\\nthey're being so stinky.\", u\"(But hey, that's competition.)\", u'-- \\n\\nStan Kerr    \\nComputing & Communications Services Office, U of Illinois/Urbana\\nPhone: 217-333-5217  Email: stankerr@uiuc.edu'] \n",
      "\n",
      "[u\"From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\\nSubject: PB questions...\\nOrganization: Purdue University Engineering Computer Network\\nDistribution: usa\\nLines: 36\\n\\nwell folks, my mac plus finally gave up the ghost this weekend after\\nstarting life as a 512k way back in 1985.  sooo, i'm in the market for a\\nnew machine a bit sooner than i intended to be...\\n\\ni'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\\nof questions that (hopefully) somebody can answer:\\n\\n* does anybody know any dirt on when the next round of powerbook\\nintroductions are expected?\", u'i\\'d heard the 185c was supposed to make an\\nappearence \"this summer\" but haven\\'t heard anymore on it - and since i\\ndon\\'t have access to macleak, i was wondering if anybody out there had\\nmore info...\\n\\n* has anybody heard rumors about price drops to the powerbook line like the\\nones the duo\\'s just went through recently?', u\"* what's the impression of the display on the 180?\", u'i could probably swing\\na 180 if i got the 80Mb disk rather than the 120, but i don\\'t really have\\na feel for how much \"better\" the display is (yea, it looks great in the\\nstore, but is that all \"wow\" or is it really that good?).', u'could i solicit\\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\\ntaking the disk size and money hit to get the active display?', u\"(i realize\\nthis is a real subjective question, but i've only played around with the\\nmachines in a computer store breifly and figured the opinions of somebody\\nwho actually uses the machine daily might prove helpful).\", u'* how well does hellcats perform?', u';)\\n\\nthanks a bunch in advance for any info - if you could email, i\\'ll post a\\nsummary (news reading time is at a premium with finals just around the\\ncorner... :( )\\n--\\nTom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical Engineering\\n---------------------------------------------------------------------------\\n\"Convictions are more dangerous enemies of truth than lies.\"', u'- F. W.\\nNietzsche'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_token_text1 = str(sent_tokenize(text1)) \n",
    "print sent_token_text1, \"\\n\"\n",
    "\n",
    "sent_token_text2 = str(sent_tokenize(text2)) \n",
    "print sent_token_text2, \"\\n\"\n",
    "\n",
    "sent_token_text3 = str(sent_tokenize(text3)) \n",
    "print sent_token_text3, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'u', \"''\", 'From', ':', 'lerxst', '@', 'wam.umd.edu', '(', 'where', \"'s\", 'my', 'thing', ')', '\\\\nSubject', ':', 'WHAT', 'car', 'is', 'this', '!', '?', '``', ',', \"u'Nntp-Posting-Host\", ':', 'rac3.wam.umd.edu\\\\nOrganization', ':', 'University', 'of', 'Maryland', ',', 'College', 'Park\\\\nLines', ':', '15\\\\n\\\\n', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw\\\\nthe', 'other', 'day', '.', \"'\", ',', \"u'It\", 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/\\\\nearly', '70s', '.', \"'\", ',', \"u'It\", 'was', 'called', 'a', 'Bricklin', '.', \"'\", ',', \"u'The\", 'doors', 'were', 'really', 'small', '.', \"'\", ',', \"u'In\", 'addition', ',', '\\\\nthe', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', \"'\", ',', \"u'This\", 'is', '\\\\nall', 'I', 'know', '.', \"'\", ',', \"u'If\", 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years\\\\nof', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you\\\\nhave', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.', \"'\", ',', \"u'Thanks\", ',', '\\\\n-', 'IL\\\\n', '--', '--', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '--', '--', \"'\", ']']\n",
      "['[', \"u'From\", ':', 'kerr', '@', 'ux1.cso.uiuc.edu', '(', 'Stan', 'Kerr', ')', '\\\\nSubject', ':', 'Re', ':', 'Sigma', 'Designs', 'Double', 'up', '?', '?', \"'\", ',', \"u'Article-I.D\", '.', \"'\", ',', 'u', \"'\", ':', 'ux1.C52u8x.B62\\\\nOrganization', ':', 'University', 'of', 'Illinois', 'at', 'Urbana\\\\nLines', ':', '29\\\\n\\\\njap10', '@', 'po.CWRU.Edu', '(', 'Joseph', 'A.', 'Pellettiere', ')', 'writes', ':', '\\\\n\\\\n\\\\n', '>', '\\\\tI', 'am', 'looking', 'for', 'any', 'information', 'about', 'the', 'Sigma', 'Designs\\\\n', '>', '\\\\tdouble', 'up', 'board', '.', \"'\", ',', \"u'All\", 'I', 'can', 'figure', 'out', 'is', 'that', 'it', 'is', 'a\\\\n', '>', '\\\\thardware', 'compression', 'board', 'that', 'works', 'with', 'AutoDoubler', ',', 'but\\\\n', '>', '\\\\tI', 'am', 'not', 'sure', 'about', 'this', '.', \"'\", ',', \"u'Also\", 'how', 'much', 'would', 'one', 'cost', '?', \"'\", ',', 'u', \"''\", 'I', \"'ve\", 'had', 'the', 'board', 'for', 'over', 'a', 'year', ',', 'and', 'it', 'does', 'work', 'with', 'Diskdoubler', ',', '\\\\nbut', 'not', 'with', 'Autodoubler', ',', 'due', 'to', 'a', 'licensing', 'problem', 'with', 'Stac', 'Technologies', ',', '\\\\nthe', 'owners', 'of', 'the', 'board', \"'s\", 'compression', 'technology', '.', '``', ',', 'u', \"''\", '(', 'I', \"'m\", 'writing', 'this\\\\nfrom', 'memory', ';', 'I', \"'ve\", 'lost', 'the', 'reference', '.', '``', ',', 'u', \"''\", 'Please', 'correct', 'me', 'if', 'I', \"'m\", 'wrong', '.', ')', \"''\", ',', 'u', \"''\", 'Using', 'the', 'board', ',', 'I', \"'ve\", 'had', 'problems', 'with', 'file', 'icons', 'being', 'lost', ',', 'but', \"it's\\\\nhard\", 'to', 'say', 'whether', 'it', \"'s\", 'the', 'board', \"'s\", 'fault', 'or', 'something', 'else', ';', 'however', ',', '\\\\nif', 'I', 'decompress', 'the', 'troubled', 'file', 'and', 'recompress', 'it', 'without', 'the', 'board', ',', '\\\\nthe', 'icon', 'usually', 'reappears', '.', '``', ',', \"u'Because\", 'of', 'the', 'above', 'mentioned', 'licensing\\\\nproblem', ',', 'the', 'freeware', 'expansion', 'utility', 'DD', 'Expand', 'will', 'not', 'decompress\\\\na', 'board-compressed', 'file', 'unless', 'you', 'have', 'the', 'board', 'installed', '.', \"'\", ',', \"u'Since\", 'Stac', 'has', 'its', 'own', 'product', 'now', ',', 'it', 'seems', 'unlikely', 'that', 'the', 'holes\\\\nin', 'Autodoubler/Diskdoubler', 'related', 'to', 'the', 'board', 'will', 'be', 'fixed', '.', \"'\", ',', 'u', \"''\", 'Which', 'is', 'sad', ',', 'and', 'makes', 'me', 'very', 'reluctant', 'to', 'buy', 'Stac', \"'s\", 'product', 'since\\\\nthey', \"'re\", 'being', 'so', 'stinky', '.', '``', ',', 'u', \"''\", '(', 'But', 'hey', ',', 'that', \"'s\", 'competition', '.', ')', \"''\", ',', 'u', \"'\", '--', '\\\\n\\\\nStan', 'Kerr', '\\\\nComputing', '&', 'Communications', 'Services', 'Office', ',', 'U', 'of', 'Illinois/Urbana\\\\nPhone', ':', '217-333-5217', 'Email', ':', 'stankerr', '@', 'uiuc.edu', \"'\", ']']\n",
      "['[', 'u', \"''\", 'From', ':', 'twillis', '@', 'ec.ecn.purdue.edu', '(', 'Thomas', 'E', 'Willis', ')', '\\\\nSubject', ':', 'PB', 'questions', '...', '\\\\nOrganization', ':', 'Purdue', 'University', 'Engineering', 'Computer', 'Network\\\\nDistribution', ':', 'usa\\\\nLines', ':', '36\\\\n\\\\nwell', 'folks', ',', 'my', 'mac', 'plus', 'finally', 'gave', 'up', 'the', 'ghost', 'this', 'weekend', 'after\\\\nstarting', 'life', 'as', 'a', '512k', 'way', 'back', 'in', '1985.', 'sooo', ',', 'i', \"'m\", 'in', 'the', 'market', 'for', 'a\\\\nnew', 'machine', 'a', 'bit', 'sooner', 'than', 'i', 'intended', 'to', 'be', '...', '\\\\n\\\\ni', \"'m\", 'looking', 'into', 'picking', 'up', 'a', 'powerbook', '160', 'or', 'maybe', '180', 'and', 'have', 'a', 'bunch\\\\nof', 'questions', 'that', '(', 'hopefully', ')', 'somebody', 'can', 'answer', ':', '\\\\n\\\\n*', 'does', 'anybody', 'know', 'any', 'dirt', 'on', 'when', 'the', 'next', 'round', 'of', 'powerbook\\\\nintroductions', 'are', 'expected', '?', '``', ',', 'u', \"'\", 'i\\\\', \"'d\", 'heard', 'the', '185c', 'was', 'supposed', 'to', 'make', 'an\\\\nappearence', '``', 'this', 'summer', \"''\", 'but', \"haven\\\\'t\", 'heard', 'anymore', 'on', 'it', '-', 'and', 'since', \"i\\\\ndon\\\\'t\", 'have', 'access', 'to', 'macleak', ',', 'i', 'was', 'wondering', 'if', 'anybody', 'out', 'there', 'had\\\\nmore', 'info', '...', '\\\\n\\\\n*', 'has', 'anybody', 'heard', 'rumors', 'about', 'price', 'drops', 'to', 'the', 'powerbook', 'line', 'like', 'the\\\\nones', 'the', 'duo\\\\', \"'s\", 'just', 'went', 'through', 'recently', '?', \"'\", ',', 'u', \"''\", '*', 'what', \"'s\", 'the', 'impression', 'of', 'the', 'display', 'on', 'the', '180', '?', '``', ',', 'u', \"'\", 'i', 'could', 'probably', 'swing\\\\na', '180', 'if', 'i', 'got', 'the', '80Mb', 'disk', 'rather', 'than', 'the', '120', ',', 'but', 'i', \"don\\\\'t\", 'really', 'have\\\\na', 'feel', 'for', 'how', 'much', '``', 'better', \"''\", 'the', 'display', 'is', '(', 'yea', ',', 'it', 'looks', 'great', 'in', 'the\\\\nstore', ',', 'but', 'is', 'that', 'all', '``', 'wow', \"''\", 'or', 'is', 'it', 'really', 'that', 'good', '?', ')', '.', \"'\", ',', \"u'could\", 'i', 'solicit\\\\nsome', 'opinions', 'of', 'people', 'who', 'use', 'the', '160', 'and', '180', 'day-to-day', 'on', 'if', 'its', 'worth\\\\ntaking', 'the', 'disk', 'size', 'and', 'money', 'hit', 'to', 'get', 'the', 'active', 'display', '?', \"'\", ',', 'u', \"''\", '(', 'i', 'realize\\\\nthis', 'is', 'a', 'real', 'subjective', 'question', ',', 'but', 'i', \"'ve\", 'only', 'played', 'around', 'with', 'the\\\\nmachines', 'in', 'a', 'computer', 'store', 'breifly', 'and', 'figured', 'the', 'opinions', 'of', 'somebody\\\\nwho', 'actually', 'uses', 'the', 'machine', 'daily', 'might', 'prove', 'helpful', ')', '.', '``', ',', \"u'*\", 'how', 'well', 'does', 'hellcats', 'perform', '?', \"'\", ',', 'u', \"'\", ';', ')', '\\\\n\\\\nthanks', 'a', 'bunch', 'in', 'advance', 'for', 'any', 'info', '-', 'if', 'you', 'could', 'email', ',', 'i\\\\', \"'ll\", 'post', 'a\\\\nsummary', '(', 'news', 'reading', 'time', 'is', 'at', 'a', 'premium', 'with', 'finals', 'just', 'around', 'the\\\\ncorner', '...', ':', '(', ')', '\\\\n', '--', '\\\\nTom', 'Willis', '\\\\\\\\', 'twillis', '@', 'ecn.purdue.edu', '\\\\\\\\', 'Purdue', 'Electrical', 'Engineering\\\\n', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '-\\\\n', \"''\", 'Convictions', 'are', 'more', 'dangerous', 'enemies', 'of', 'truth', 'than', 'lies', '.', '``', \"'\", ',', \"u'-\", 'F.', 'W.\\\\nNietzsche', \"'\", ']']\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the text after sentence splitting\n",
    "word_sent_token_text1 = str(word_tokenize(sent_token_text1))\n",
    "print word_sent_token_text1\n",
    "\n",
    "word_sent_token_text2 = str(word_tokenize(sent_token_text2))\n",
    "print word_sent_token_text2\n",
    "\n",
    "word_sent_token_text3 = str(word_tokenize(sent_token_text3))\n",
    "print word_sent_token_text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'From', u'lerxst', u'wam', u'umd', u'edu', u'where', u's', u'my', u'thing', u'Subject', u'WHAT', u'car', u'is', u'this', u'Nntp', u'Posting', u'Host', u'rac3', u'wam', u'umd', u'edu', u'Organization', u'University', u'of', u'Maryland', u'College', u'Park', u'Lines', u'15', u'I', u'was', u'wondering', u'if', u'anyone', u'out', u'there', u'could', u'enlighten', u'me', u'on', u'this', u'car', u'I', u'saw', u'the', u'other', u'day', u'It', u'was', u'a', u'2', u'door', u'sports', u'car', u'looked', u'to', u'be', u'from', u'the', u'late', u'60s', u'early', u'70s', u'It', u'was', u'called', u'a', u'Bricklin', u'The', u'doors', u'were', u'really', u'small', u'In', u'addition', u'the', u'front', u'bumper', u'was', u'separate', u'from', u'the', u'rest', u'of', u'the', u'body', u'This', u'is', u'all', u'I', u'know', u'If', u'anyone', u'can', u'tellme', u'a', u'model', u'name', u'engine', u'specs', u'years', u'of', u'production', u'where', u'this', u'car', u'is', u'made', u'history', u'or', u'whatever', u'info', u'you', u'have', u'on', u'this', u'funky', u'looking', u'car', u'please', u'e', u'mail', u'Thanks', u'IL', u'brought', u'to', u'you', u'by', u'your', u'neighborhood', u'Lerxst'] \n",
      "\n",
      "[u'From', u'kerr', u'ux1', u'cso', u'uiuc', u'edu', u'Stan', u'Kerr', u'Subject', u'Re', u'Sigma', u'Designs', u'Double', u'up', u'Article', u'I', u'D', u'ux1', u'C52u8x', u'B62', u'Organization', u'University', u'of', u'Illinois', u'at', u'Urbana', u'Lines', u'29', u'jap10', u'po', u'CWRU', u'Edu', u'Joseph', u'A', u'Pellettiere', u'writes', u'I', u'am', u'looking', u'for', u'any', u'information', u'about', u'the', u'Sigma', u'Designs', u'double', u'up', u'board', u'All', u'I', u'can', u'figure', u'out', u'is', u'that', u'it', u'is', u'a', u'hardware', u'compression', u'board', u'that', u'works', u'with', u'AutoDoubler', u'but', u'I', u'am', u'not', u'sure', u'about', u'this', u'Also', u'how', u'much', u'would', u'one', u'cost', u'I', u've', u'had', u'the', u'board', u'for', u'over', u'a', u'year', u'and', u'it', u'does', u'work', u'with', u'Diskdoubler', u'but', u'not', u'with', u'Autodoubler', u'due', u'to', u'a', u'licensing', u'problem', u'with', u'Stac', u'Technologies', u'the', u'owners', u'of', u'the', u'board', u's', u'compression', u'technology', u'I', u'm', u'writing', u'this', u'from', u'memory', u'I', u've', u'lost', u'the', u'reference', u'Please', u'correct', u'me', u'if', u'I', u'm', u'wrong', u'Using', u'the', u'board', u'I', u've', u'had', u'problems', u'with', u'file', u'icons', u'being', u'lost', u'but', u'it', u's', u'hard', u'to', u'say', u'whether', u'it', u's', u'the', u'board', u's', u'fault', u'or', u'something', u'else', u'however', u'if', u'I', u'decompress', u'the', u'troubled', u'file', u'and', u'recompress', u'it', u'without', u'the', u'board', u'the', u'icon', u'usually', u'reappears', u'Because', u'of', u'the', u'above', u'mentioned', u'licensing', u'problem', u'the', u'freeware', u'expansion', u'utility', u'DD', u'Expand', u'will', u'not', u'decompress', u'a', u'board', u'compressed', u'file', u'unless', u'you', u'have', u'the', u'board', u'installed', u'Since', u'Stac', u'has', u'its', u'own', u'product', u'now', u'it', u'seems', u'unlikely', u'that', u'the', u'holes', u'in', u'Autodoubler', u'Diskdoubler', u'related', u'to', u'the', u'board', u'will', u'be', u'fixed', u'Which', u'is', u'sad', u'and', u'makes', u'me', u'very', u'reluctant', u'to', u'buy', u'Stac', u's', u'product', u'since', u'they', u're', u'being', u'so', u'stinky', u'But', u'hey', u'that', u's', u'competition', u'Stan', u'Kerr', u'Computing', u'Communications', u'Services', u'Office', u'U', u'of', u'Illinois', u'Urbana', u'Phone', u'217', u'333', u'5217', u'Email', u'stankerr', u'uiuc', u'edu'] \n",
      "\n",
      "([u'From', u'twillis', u'ec', u'ecn', u'purdue', u'edu', u'Thomas', u'E', u'Willis', u'Subject', u'PB', u'questions', u'Organization', u'Purdue', u'University', u'Engineering', u'Computer', u'Network', u'Distribution', u'usa', u'Lines', u'36', u'well', u'folks', u'my', u'mac', u'plus', u'finally', u'gave', u'up', u'the', u'ghost', u'this', u'weekend', u'after', u'starting', u'life', u'as', u'a', u'512k', u'way', u'back', u'in', u'1985', u'sooo', u'i', u'm', u'in', u'the', u'market', u'for', u'a', u'new', u'machine', u'a', u'bit', u'sooner', u'than', u'i', u'intended', u'to', u'be', u'i', u'm', u'looking', u'into', u'picking', u'up', u'a', u'powerbook', u'160', u'or', u'maybe', u'180', u'and', u'have', u'a', u'bunch', u'of', u'questions', u'that', u'hopefully', u'somebody', u'can', u'answer', u'does', u'anybody', u'know', u'any', u'dirt', u'on', u'when', u'the', u'next', u'round', u'of', u'powerbook', u'introductions', u'are', u'expected', u'i', u'd', u'heard', u'the', u'185c', u'was', u'supposed', u'to', u'make', u'an', u'appearence', u'this', u'summer', u'but', u'haven', u't', u'heard', u'anymore', u'on', u'it', u'and', u'since', u'i', u'don', u't', u'have', u'access', u'to', u'macleak', u'i', u'was', u'wondering', u'if', u'anybody', u'out', u'there', u'had', u'more', u'info', u'has', u'anybody', u'heard', u'rumors', u'about', u'price', u'drops', u'to', u'the', u'powerbook', u'line', u'like', u'the', u'ones', u'the', u'duo', u's', u'just', u'went', u'through', u'recently', u'what', u's', u'the', u'impression', u'of', u'the', u'display', u'on', u'the', u'180', u'i', u'could', u'probably', u'swing', u'a', u'180', u'if', u'i', u'got', u'the', u'80Mb', u'disk', u'rather', u'than', u'the', u'120', u'but', u'i', u'don', u't', u'really', u'have', u'a', u'feel', u'for', u'how', u'much', u'better', u'the', u'display', u'is', u'yea', u'it', u'looks', u'great', u'in', u'the', u'store', u'but', u'is', u'that', u'all', u'wow', u'or', u'is', u'it', u'really', u'that', u'good', u'could', u'i', u'solicit', u'some', u'opinions', u'of', u'people', u'who', u'use', u'the', u'160', u'and', u'180', u'day', u'to', u'day', u'on', u'if', u'its', u'worth', u'taking', u'the', u'disk', u'size', u'and', u'money', u'hit', u'to', u'get', u'the', u'active', u'display', u'i', u'realize', u'this', u'is', u'a', u'real', u'subjective', u'question', u'but', u'i', u've', u'only', u'played', u'around', u'with', u'the', u'machines', u'in', u'a', u'computer', u'store', u'breifly', u'and', u'figured', u'the', u'opinions', u'of', u'somebody', u'who', u'actually', u'uses', u'the', u'machine', u'daily', u'might', u'prove', u'helpful', u'how', u'well', u'does', u'hellcats', u'perform', u'thanks', u'a', u'bunch', u'in', u'advance', u'for', u'any', u'info', u'if', u'you', u'could', u'email', u'i', u'll', u'post', u'a', u'summary', u'news', u'reading', u'time', u'is', u'at', u'a', u'premium', u'with', u'finals', u'just', u'around', u'the', u'corner', u'Tom', u'Willis', u'twillis', u'ecn', u'purdue', u'edu', u'Purdue', u'Electrical', u'Engineering', u'Convictions', u'are', u'more', u'dangerous', u'enemies', u'of', u'truth', u'than', u'lies', u'F', u'W', u'Nietzsche'], '\\n')\n"
     ]
    }
   ],
   "source": [
    "#RegexpTokenizer to remove the punctuations etc after tokenizing\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "word_sent_token_text1 = (tokenizer.tokenize(text1))\n",
    "print (word_sent_token_text1), \"\\n\"\n",
    "\n",
    "word_sent_token_text2 = (tokenizer.tokenize(text2))\n",
    "print (word_sent_token_text2), \"\\n\"\n",
    "\n",
    "word_sent_token_text3 = (tokenizer.tokenize(text3))\n",
    "print (word_sent_token_text3, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sreeganesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# WordNetLemmatizer to get a cautious version of stemming.\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'From', u'lerxst', u'wam', u'umd', u'edu', u'where', u's', u'my', u'thing', u'Subject', u'WHAT', u'car', u'is', u'this', u'Nntp', u'Posting', u'Host', u'rac3', u'wam', u'umd', u'edu', u'Organization', u'University', u'of', u'Maryland', u'College', u'Park', u'Lines', u'15', u'I', u'was', u'wondering', u'if', u'anyone', u'out', u'there', u'could', u'enlighten', u'me', u'on', u'this', u'car', u'I', u'saw', u'the', u'other', u'day', u'It', u'was', u'a', u'2', u'door', u'sports', u'car', u'looked', u'to', u'be', u'from', u'the', u'late', u'60s', u'early', u'70s', u'It', u'was', u'called', u'a', u'Bricklin', u'The', u'doors', u'were', u'really', u'small', u'In', u'addition', u'the', u'front', u'bumper', u'was', u'separate', u'from', u'the', u'rest', u'of', u'the', u'body', u'This', u'is', u'all', u'I', u'know', u'If', u'anyone', u'can', u'tellme', u'a', u'model', u'name', u'engine', u'specs', u'years', u'of', u'production', u'where', u'this', u'car', u'is', u'made', u'history', u'or', u'whatever', u'info', u'you', u'have', u'on', u'this', u'funky', u'looking', u'car', u'please', u'e', u'mail', u'Thanks', u'IL', u'brought', u'to', u'you', u'by', u'your', u'neighborhood', u'Lerxst']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize_word_sent_token_text1 = str(lemmatizer.lemmatize(word_sent_token_text1)) \n",
    "# print lemmatize_word_sent_token_text1\n",
    "lemmatize_word_sent_token_text1 = []\n",
    "for i in range(0, len(word_sent_token_text1)):\n",
    "     l1 = word_sent_token_text1[i]\n",
    "     l2 = ''.join([lemmatizer.lemmatize(word) for word in l1])\n",
    "     lemmatize_word_sent_token_text1.append(l2)\n",
    "print lemmatize_word_sent_token_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FIle 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'From', u'kerr', u'ux1', u'cso', u'uiuc', u'edu', u'Stan', u'Kerr', u'Subject', u'Re', u'Sigma', u'Designs', u'Double', u'up', u'Article', u'I', u'D', u'ux1', u'C52u8x', u'B62', u'Organization', u'University', u'of', u'Illinois', u'at', u'Urbana', u'Lines', u'29', u'jap10', u'po', u'CWRU', u'Edu', u'Joseph', u'A', u'Pellettiere', u'writes', u'I', u'am', u'looking', u'for', u'any', u'information', u'about', u'the', u'Sigma', u'Designs', u'double', u'up', u'board', u'All', u'I', u'can', u'figure', u'out', u'is', u'that', u'it', u'is', u'a', u'hardware', u'compression', u'board', u'that', u'works', u'with', u'AutoDoubler', u'but', u'I', u'am', u'not', u'sure', u'about', u'this', u'Also', u'how', u'much', u'would', u'one', u'cost', u'I', u've', u'had', u'the', u'board', u'for', u'over', u'a', u'year', u'and', u'it', u'does', u'work', u'with', u'Diskdoubler', u'but', u'not', u'with', u'Autodoubler', u'due', u'to', u'a', u'licensing', u'problem', u'with', u'Stac', u'Technologies', u'the', u'owners', u'of', u'the', u'board', u's', u'compression', u'technology', u'I', u'm', u'writing', u'this', u'from', u'memory', u'I', u've', u'lost', u'the', u'reference', u'Please', u'correct', u'me', u'if', u'I', u'm', u'wrong', u'Using', u'the', u'board', u'I', u've', u'had', u'problems', u'with', u'file', u'icons', u'being', u'lost', u'but', u'it', u's', u'hard', u'to', u'say', u'whether', u'it', u's', u'the', u'board', u's', u'fault', u'or', u'something', u'else', u'however', u'if', u'I', u'decompress', u'the', u'troubled', u'file', u'and', u'recompress', u'it', u'without', u'the', u'board', u'the', u'icon', u'usually', u'reappears', u'Because', u'of', u'the', u'above', u'mentioned', u'licensing', u'problem', u'the', u'freeware', u'expansion', u'utility', u'DD', u'Expand', u'will', u'not', u'decompress', u'a', u'board', u'compressed', u'file', u'unless', u'you', u'have', u'the', u'board', u'installed', u'Since', u'Stac', u'has', u'its', u'own', u'product', u'now', u'it', u'seems', u'unlikely', u'that', u'the', u'holes', u'in', u'Autodoubler', u'Diskdoubler', u'related', u'to', u'the', u'board', u'will', u'be', u'fixed', u'Which', u'is', u'sad', u'and', u'makes', u'me', u'very', u'reluctant', u'to', u'buy', u'Stac', u's', u'product', u'since', u'they', u're', u'being', u'so', u'stinky', u'But', u'hey', u'that', u's', u'competition', u'Stan', u'Kerr', u'Computing', u'Communications', u'Services', u'Office', u'U', u'of', u'Illinois', u'Urbana', u'Phone', u'217', u'333', u'5217', u'Email', u'stankerr', u'uiuc', u'edu']\n"
     ]
    }
   ],
   "source": [
    "lemmatize_word_sent_token_text2 = []\n",
    "for i in range(0, len(word_sent_token_text2)):\n",
    "     l1 = word_sent_token_text2[i]\n",
    "     l2 = ''.join([lemmatizer.lemmatize(word) for word in l1])\n",
    "     lemmatize_word_sent_token_text2.append(l2)\n",
    "print lemmatize_word_sent_token_text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'From', u'twillis', u'ec', u'ecn', u'purdue', u'edu', u'Thomas', u'E', u'Willis', u'Subject', u'PB', u'questions', u'Organization', u'Purdue', u'University', u'Engineering', u'Computer', u'Network', u'Distribution', u'usa', u'Lines', u'36', u'well', u'folks', u'my', u'mac', u'plus', u'finally', u'gave', u'up', u'the', u'ghost', u'this', u'weekend', u'after', u'starting', u'life', u'as', u'a', u'512k', u'way', u'back', u'in', u'1985', u'sooo', u'i', u'm', u'in', u'the', u'market', u'for', u'a', u'new', u'machine', u'a', u'bit', u'sooner', u'than', u'i', u'intended', u'to', u'be', u'i', u'm', u'looking', u'into', u'picking', u'up', u'a', u'powerbook', u'160', u'or', u'maybe', u'180', u'and', u'have', u'a', u'bunch', u'of', u'questions', u'that', u'hopefully', u'somebody', u'can', u'answer', u'does', u'anybody', u'know', u'any', u'dirt', u'on', u'when', u'the', u'next', u'round', u'of', u'powerbook', u'introductions', u'are', u'expected', u'i', u'd', u'heard', u'the', u'185c', u'was', u'supposed', u'to', u'make', u'an', u'appearence', u'this', u'summer', u'but', u'haven', u't', u'heard', u'anymore', u'on', u'it', u'and', u'since', u'i', u'don', u't', u'have', u'access', u'to', u'macleak', u'i', u'was', u'wondering', u'if', u'anybody', u'out', u'there', u'had', u'more', u'info', u'has', u'anybody', u'heard', u'rumors', u'about', u'price', u'drops', u'to', u'the', u'powerbook', u'line', u'like', u'the', u'ones', u'the', u'duo', u's', u'just', u'went', u'through', u'recently', u'what', u's', u'the', u'impression', u'of', u'the', u'display', u'on', u'the', u'180', u'i', u'could', u'probably', u'swing', u'a', u'180', u'if', u'i', u'got', u'the', u'80Mb', u'disk', u'rather', u'than', u'the', u'120', u'but', u'i', u'don', u't', u'really', u'have', u'a', u'feel', u'for', u'how', u'much', u'better', u'the', u'display', u'is', u'yea', u'it', u'looks', u'great', u'in', u'the', u'store', u'but', u'is', u'that', u'all', u'wow', u'or', u'is', u'it', u'really', u'that', u'good', u'could', u'i', u'solicit', u'some', u'opinions', u'of', u'people', u'who', u'use', u'the', u'160', u'and', u'180', u'day', u'to', u'day', u'on', u'if', u'its', u'worth', u'taking', u'the', u'disk', u'size', u'and', u'money', u'hit', u'to', u'get', u'the', u'active', u'display', u'i', u'realize', u'this', u'is', u'a', u'real', u'subjective', u'question', u'but', u'i', u've', u'only', u'played', u'around', u'with', u'the', u'machines', u'in', u'a', u'computer', u'store', u'breifly', u'and', u'figured', u'the', u'opinions', u'of', u'somebody', u'who', u'actually', u'uses', u'the', u'machine', u'daily', u'might', u'prove', u'helpful', u'how', u'well', u'does', u'hellcats', u'perform', u'thanks', u'a', u'bunch', u'in', u'advance', u'for', u'any', u'info', u'if', u'you', u'could', u'email', u'i', u'll', u'post', u'a', u'summary', u'news', u'reading', u'time', u'is', u'at', u'a', u'premium', u'with', u'finals', u'just', u'around', u'the', u'corner', u'Tom', u'Willis', u'twillis', u'ecn', u'purdue', u'edu', u'Purdue', u'Electrical', u'Engineering', u'Convictions', u'are', u'more', u'dangerous', u'enemies', u'of', u'truth', u'than', u'lies', u'F', u'W', u'Nietzsche']\n"
     ]
    }
   ],
   "source": [
    "lemmatize_word_sent_token_text3 = []\n",
    "for i in range(0, len(word_sent_token_text3)):\n",
    "     l1 = word_sent_token_text3[i]\n",
    "     l2 = ''.join([lemmatizer.lemmatize(word) for word in l1])\n",
    "     lemmatize_word_sent_token_text3.append(l2)\n",
    "print lemmatize_word_sent_token_text3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sreeganesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing Stopwords: \n",
      "\n",
      "['From', 'lerxst', 'wam', 'umd', 'edu', 'where', 's', 'my', 'thing', 'Subject', 'WHAT', 'car', 'is', 'this', 'Nntp', 'Posting', 'Host', 'rac3', 'wam', 'umd', 'edu', 'Organization', 'University', 'of', 'Maryland', 'College', 'Park', 'Lines', '15', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', 'It', 'was', 'a', '2', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', '60s', 'early', '70s', 'It', 'was', 'called', 'a', 'Bricklin', 'The', 'doors', 'were', 'really', 'small', 'In', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'This', 'is', 'all', 'I', 'know', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'e', 'mail', 'Thanks', 'IL', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst']\n",
      "\n",
      "After Stopwords removal\n",
      "['From', 'lerxst', 'wam', 'umd', 'edu', 'thing', 'Subject', 'WHAT', 'car', 'Nntp', 'Posting', 'Host', 'rac3', 'wam', 'umd', 'edu', 'Organization', 'University', 'Maryland', 'College', 'Park', 'Lines', '15', 'I', 'wondering', 'anyone', 'could', 'enlighten', 'car', 'I', 'saw', 'day', 'It', '2', 'door', 'sports', 'car', 'looked', 'late', '60s', 'early', '70s', 'It', 'called', 'Bricklin', 'The', 'doors', 'really', 'small', 'In', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'This', 'I', 'know', 'If', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please', 'e', 'mail', 'Thanks', 'IL', 'brought', 'neighborhood', 'Lerxst']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = lemmatize_word_sent_token_text1\n",
    "filtered_text1 = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_text1 = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_text1.append(w) \n",
    "\n",
    "#removing the character 'u' (representing unicode) and printing the         \n",
    "word_tokens1 = [str(r) for r in word_tokens]  \n",
    "print (\"Before removing Stopwords: \\n\")\n",
    "print(word_tokens1) \n",
    "print(\"\\nAfter Stopwords removal\")\n",
    "filtered_text_var1 = [str(r) for r in filtered_text1]  \n",
    "print(filtered_text_var1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing Stopwords: \n",
      "\n",
      "['From', 'kerr', 'ux1', 'cso', 'uiuc', 'edu', 'Stan', 'Kerr', 'Subject', 'Re', 'Sigma', 'Designs', 'Double', 'up', 'Article', 'I', 'D', 'ux1', 'C52u8x', 'B62', 'Organization', 'University', 'of', 'Illinois', 'at', 'Urbana', 'Lines', '29', 'jap10', 'po', 'CWRU', 'Edu', 'Joseph', 'A', 'Pellettiere', 'writes', 'I', 'am', 'looking', 'for', 'any', 'information', 'about', 'the', 'Sigma', 'Designs', 'double', 'up', 'board', 'All', 'I', 'can', 'figure', 'out', 'is', 'that', 'it', 'is', 'a', 'hardware', 'compression', 'board', 'that', 'works', 'with', 'AutoDoubler', 'but', 'I', 'am', 'not', 'sure', 'about', 'this', 'Also', 'how', 'much', 'would', 'one', 'cost', 'I', 've', 'had', 'the', 'board', 'for', 'over', 'a', 'year', 'and', 'it', 'does', 'work', 'with', 'Diskdoubler', 'but', 'not', 'with', 'Autodoubler', 'due', 'to', 'a', 'licensing', 'problem', 'with', 'Stac', 'Technologies', 'the', 'owners', 'of', 'the', 'board', 's', 'compression', 'technology', 'I', 'm', 'writing', 'this', 'from', 'memory', 'I', 've', 'lost', 'the', 'reference', 'Please', 'correct', 'me', 'if', 'I', 'm', 'wrong', 'Using', 'the', 'board', 'I', 've', 'had', 'problems', 'with', 'file', 'icons', 'being', 'lost', 'but', 'it', 's', 'hard', 'to', 'say', 'whether', 'it', 's', 'the', 'board', 's', 'fault', 'or', 'something', 'else', 'however', 'if', 'I', 'decompress', 'the', 'troubled', 'file', 'and', 'recompress', 'it', 'without', 'the', 'board', 'the', 'icon', 'usually', 'reappears', 'Because', 'of', 'the', 'above', 'mentioned', 'licensing', 'problem', 'the', 'freeware', 'expansion', 'utility', 'DD', 'Expand', 'will', 'not', 'decompress', 'a', 'board', 'compressed', 'file', 'unless', 'you', 'have', 'the', 'board', 'installed', 'Since', 'Stac', 'has', 'its', 'own', 'product', 'now', 'it', 'seems', 'unlikely', 'that', 'the', 'holes', 'in', 'Autodoubler', 'Diskdoubler', 'related', 'to', 'the', 'board', 'will', 'be', 'fixed', 'Which', 'is', 'sad', 'and', 'makes', 'me', 'very', 'reluctant', 'to', 'buy', 'Stac', 's', 'product', 'since', 'they', 're', 'being', 'so', 'stinky', 'But', 'hey', 'that', 's', 'competition', 'Stan', 'Kerr', 'Computing', 'Communications', 'Services', 'Office', 'U', 'of', 'Illinois', 'Urbana', 'Phone', '217', '333', '5217', 'Email', 'stankerr', 'uiuc', 'edu']\n",
      "\n",
      "After Stopwords removal\n",
      "['From', 'kerr', 'ux1', 'cso', 'uiuc', 'edu', 'Stan', 'Kerr', 'Subject', 'Re', 'Sigma', 'Designs', 'Double', 'Article', 'I', 'D', 'ux1', 'C52u8x', 'B62', 'Organization', 'University', 'Illinois', 'Urbana', 'Lines', '29', 'jap10', 'po', 'CWRU', 'Edu', 'Joseph', 'A', 'Pellettiere', 'writes', 'I', 'looking', 'information', 'Sigma', 'Designs', 'double', 'board', 'All', 'I', 'figure', 'hardware', 'compression', 'board', 'works', 'AutoDoubler', 'I', 'sure', 'Also', 'much', 'would', 'one', 'cost', 'I', 'board', 'year', 'work', 'Diskdoubler', 'Autodoubler', 'due', 'licensing', 'problem', 'Stac', 'Technologies', 'owners', 'board', 'compression', 'technology', 'I', 'writing', 'memory', 'I', 'lost', 'reference', 'Please', 'correct', 'I', 'wrong', 'Using', 'board', 'I', 'problems', 'file', 'icons', 'lost', 'hard', 'say', 'whether', 'board', 'fault', 'something', 'else', 'however', 'I', 'decompress', 'troubled', 'file', 'recompress', 'without', 'board', 'icon', 'usually', 'reappears', 'Because', 'mentioned', 'licensing', 'problem', 'freeware', 'expansion', 'utility', 'DD', 'Expand', 'decompress', 'board', 'compressed', 'file', 'unless', 'board', 'installed', 'Since', 'Stac', 'product', 'seems', 'unlikely', 'holes', 'Autodoubler', 'Diskdoubler', 'related', 'board', 'fixed', 'Which', 'sad', 'makes', 'reluctant', 'buy', 'Stac', 'product', 'since', 'stinky', 'But', 'hey', 'competition', 'Stan', 'Kerr', 'Computing', 'Communications', 'Services', 'Office', 'U', 'Illinois', 'Urbana', 'Phone', '217', '333', '5217', 'Email', 'stankerr', 'uiuc', 'edu']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = lemmatize_word_sent_token_text2\n",
    "filtered_text2 = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_text2 = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_text2.append(w) \n",
    "\n",
    "#removing the character 'u' (representing unicode) and printing the         \n",
    "word_tokens2 = [str(r) for r in word_tokens]  \n",
    "print (\"Before removing Stopwords: \\n\")\n",
    "print(word_tokens2) \n",
    "print(\"\\nAfter Stopwords removal\")\n",
    "filtered_text_var2 = [str(r) for r in filtered_text2]  \n",
    "print(filtered_text_var2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing Stopwords: \n",
      "\n",
      "['From', 'twillis', 'ec', 'ecn', 'purdue', 'edu', 'Thomas', 'E', 'Willis', 'Subject', 'PB', 'questions', 'Organization', 'Purdue', 'University', 'Engineering', 'Computer', 'Network', 'Distribution', 'usa', 'Lines', '36', 'well', 'folks', 'my', 'mac', 'plus', 'finally', 'gave', 'up', 'the', 'ghost', 'this', 'weekend', 'after', 'starting', 'life', 'as', 'a', '512k', 'way', 'back', 'in', '1985', 'sooo', 'i', 'm', 'in', 'the', 'market', 'for', 'a', 'new', 'machine', 'a', 'bit', 'sooner', 'than', 'i', 'intended', 'to', 'be', 'i', 'm', 'looking', 'into', 'picking', 'up', 'a', 'powerbook', '160', 'or', 'maybe', '180', 'and', 'have', 'a', 'bunch', 'of', 'questions', 'that', 'hopefully', 'somebody', 'can', 'answer', 'does', 'anybody', 'know', 'any', 'dirt', 'on', 'when', 'the', 'next', 'round', 'of', 'powerbook', 'introductions', 'are', 'expected', 'i', 'd', 'heard', 'the', '185c', 'was', 'supposed', 'to', 'make', 'an', 'appearence', 'this', 'summer', 'but', 'haven', 't', 'heard', 'anymore', 'on', 'it', 'and', 'since', 'i', 'don', 't', 'have', 'access', 'to', 'macleak', 'i', 'was', 'wondering', 'if', 'anybody', 'out', 'there', 'had', 'more', 'info', 'has', 'anybody', 'heard', 'rumors', 'about', 'price', 'drops', 'to', 'the', 'powerbook', 'line', 'like', 'the', 'ones', 'the', 'duo', 's', 'just', 'went', 'through', 'recently', 'what', 's', 'the', 'impression', 'of', 'the', 'display', 'on', 'the', '180', 'i', 'could', 'probably', 'swing', 'a', '180', 'if', 'i', 'got', 'the', '80Mb', 'disk', 'rather', 'than', 'the', '120', 'but', 'i', 'don', 't', 'really', 'have', 'a', 'feel', 'for', 'how', 'much', 'better', 'the', 'display', 'is', 'yea', 'it', 'looks', 'great', 'in', 'the', 'store', 'but', 'is', 'that', 'all', 'wow', 'or', 'is', 'it', 'really', 'that', 'good', 'could', 'i', 'solicit', 'some', 'opinions', 'of', 'people', 'who', 'use', 'the', '160', 'and', '180', 'day', 'to', 'day', 'on', 'if', 'its', 'worth', 'taking', 'the', 'disk', 'size', 'and', 'money', 'hit', 'to', 'get', 'the', 'active', 'display', 'i', 'realize', 'this', 'is', 'a', 'real', 'subjective', 'question', 'but', 'i', 've', 'only', 'played', 'around', 'with', 'the', 'machines', 'in', 'a', 'computer', 'store', 'breifly', 'and', 'figured', 'the', 'opinions', 'of', 'somebody', 'who', 'actually', 'uses', 'the', 'machine', 'daily', 'might', 'prove', 'helpful', 'how', 'well', 'does', 'hellcats', 'perform', 'thanks', 'a', 'bunch', 'in', 'advance', 'for', 'any', 'info', 'if', 'you', 'could', 'email', 'i', 'll', 'post', 'a', 'summary', 'news', 'reading', 'time', 'is', 'at', 'a', 'premium', 'with', 'finals', 'just', 'around', 'the', 'corner', 'Tom', 'Willis', 'twillis', 'ecn', 'purdue', 'edu', 'Purdue', 'Electrical', 'Engineering', 'Convictions', 'are', 'more', 'dangerous', 'enemies', 'of', 'truth', 'than', 'lies', 'F', 'W', 'Nietzsche']\n",
      "\n",
      "After Stopwords removal\n",
      "['From', 'twillis', 'ec', 'ecn', 'purdue', 'edu', 'Thomas', 'E', 'Willis', 'Subject', 'PB', 'questions', 'Organization', 'Purdue', 'University', 'Engineering', 'Computer', 'Network', 'Distribution', 'usa', 'Lines', '36', 'well', 'folks', 'mac', 'plus', 'finally', 'gave', 'ghost', 'weekend', 'starting', 'life', '512k', 'way', 'back', '1985', 'sooo', 'market', 'new', 'machine', 'bit', 'sooner', 'intended', 'looking', 'picking', 'powerbook', '160', 'maybe', '180', 'bunch', 'questions', 'hopefully', 'somebody', 'answer', 'anybody', 'know', 'dirt', 'next', 'round', 'powerbook', 'introductions', 'expected', 'heard', '185c', 'supposed', 'make', 'appearence', 'summer', 'heard', 'anymore', 'since', 'access', 'macleak', 'wondering', 'anybody', 'info', 'anybody', 'heard', 'rumors', 'price', 'drops', 'powerbook', 'line', 'like', 'ones', 'duo', 'went', 'recently', 'impression', 'display', '180', 'could', 'probably', 'swing', '180', 'got', '80Mb', 'disk', 'rather', '120', 'really', 'feel', 'much', 'better', 'display', 'yea', 'looks', 'great', 'store', 'wow', 'really', 'good', 'could', 'solicit', 'opinions', 'people', 'use', '160', '180', 'day', 'day', 'worth', 'taking', 'disk', 'size', 'money', 'hit', 'get', 'active', 'display', 'realize', 'real', 'subjective', 'question', 'played', 'around', 'machines', 'computer', 'store', 'breifly', 'figured', 'opinions', 'somebody', 'actually', 'uses', 'machine', 'daily', 'might', 'prove', 'helpful', 'well', 'hellcats', 'perform', 'thanks', 'bunch', 'advance', 'info', 'could', 'email', 'post', 'summary', 'news', 'reading', 'time', 'premium', 'finals', 'around', 'corner', 'Tom', 'Willis', 'twillis', 'ecn', 'purdue', 'edu', 'Purdue', 'Electrical', 'Engineering', 'Convictions', 'dangerous', 'enemies', 'truth', 'lies', 'F', 'W', 'Nietzsche']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = lemmatize_word_sent_token_text3\n",
    "filtered_text3 = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_text3 = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_text3.append(w) \n",
    "\n",
    "#removing the character 'u' (representing unicode) and printing the         \n",
    "word_tokens3 = [str(r) for r in word_tokens]  \n",
    "print (\"Before removing Stopwords: \\n\")\n",
    "print(word_tokens3) \n",
    "print(\"\\nAfter Stopwords removal\")\n",
    "filtered_text_var3 = [str(r) for r in filtered_text3]  \n",
    "print(filtered_text_var3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sreeganesh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the built-in tagging function pos_tag\n",
    "from nltk.tag import pos_tag \n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('From', 'IN'), ('lerxst', 'JJ'), ('wam', 'NN'), ('umd', 'JJ'), ('edu', 'JJ'), ('thing', 'NN'), ('Subject', 'NNP'), ('WHAT', 'NNP'), ('car', 'NN'), ('Nntp', 'NNP'), ('Posting', 'NNP'), ('Host', 'NNP'), ('rac3', 'NN'), ('wam', 'NN'), ('umd', 'JJ'), ('edu', 'JJ'), ('Organization', 'NNP'), ('University', 'NNP'), ('Maryland', 'NNP'), ('College', 'NNP'), ('Park', 'NNP'), ('Lines', 'NNP'), ('15', 'CD'), ('I', 'PRP'), ('wondering', 'VBG'), ('anyone', 'NN'), ('could', 'MD'), ('enlighten', 'VB'), ('car', 'NN'), ('I', 'PRP'), ('saw', 'VBD'), ('day', 'NN'), ('It', 'PRP'), ('2', 'CD'), ('door', 'JJ'), ('sports', 'NNS'), ('car', 'NN'), ('looked', 'VBD'), ('late', 'RB'), ('60s', 'CD'), ('early', 'JJ'), ('70s', 'CD'), ('It', 'PRP'), ('called', 'VBD'), ('Bricklin', 'NNP'), ('The', 'DT'), ('doors', 'NNS'), ('really', 'RB'), ('small', 'JJ'), ('In', 'IN'), ('addition', 'NN'), ('front', 'JJ'), ('bumper', 'NN'), ('separate', 'JJ'), ('rest', 'NN'), ('body', 'NN'), ('This', 'DT'), ('I', 'PRP'), ('know', 'VBP'), ('If', 'IN'), ('anyone', 'NN'), ('tellme', 'NN'), ('model', 'NN'), ('name', 'NN'), ('engine', 'NN'), ('specs', 'NN'), ('years', 'NNS'), ('production', 'NN'), ('car', 'NN'), ('made', 'VBD'), ('history', 'NN'), ('whatever', 'WDT'), ('info', 'NN'), ('funky', 'NN'), ('looking', 'VBG'), ('car', 'NN'), ('please', 'NN'), ('e', 'JJ'), ('mail', 'NN'), ('Thanks', 'NNP'), ('IL', 'NNP'), ('brought', 'VBD'), ('neighborhood', 'NN'), ('Lerxst', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tag_filtered_text1 = nltk.pos_tag(filtered_text_var1)\n",
    "print tag_filtered_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('From', 'IN'), ('kerr', 'VBN'), ('ux1', 'JJ'), ('cso', 'NN'), ('uiuc', 'JJ'), ('edu', 'NN'), ('Stan', 'NNP'), ('Kerr', 'NNP'), ('Subject', 'NNP'), ('Re', 'NNP'), ('Sigma', 'NNP'), ('Designs', 'NNP'), ('Double', 'NNP'), ('Article', 'NNP'), ('I', 'PRP'), ('D', 'NNP'), ('ux1', 'JJ'), ('C52u8x', 'NNP'), ('B62', 'NNP'), ('Organization', 'NNP'), ('University', 'NNP'), ('Illinois', 'NNP'), ('Urbana', 'NNP'), ('Lines', 'NNP'), ('29', 'CD'), ('jap10', 'NN'), ('po', 'NN'), ('CWRU', 'NNP'), ('Edu', 'NNP'), ('Joseph', 'NNP'), ('A', 'NNP'), ('Pellettiere', 'NNP'), ('writes', 'VBZ'), ('I', 'PRP'), ('looking', 'VBG'), ('information', 'NN'), ('Sigma', 'NNP'), ('Designs', 'NNP'), ('double', 'RB'), ('board', 'NN'), ('All', 'DT'), ('I', 'PRP'), ('figure', 'VBP'), ('hardware', 'JJ'), ('compression', 'NN'), ('board', 'NN'), ('works', 'VBZ'), ('AutoDoubler', 'NNP'), ('I', 'PRP'), ('sure', 'VBP'), ('Also', 'RB'), ('much', 'RB'), ('would', 'MD'), ('one', 'CD'), ('cost', 'NN'), ('I', 'PRP'), ('board', 'NN'), ('year', 'NN'), ('work', 'NN'), ('Diskdoubler', 'NNP'), ('Autodoubler', 'NNP'), ('due', 'JJ'), ('licensing', 'NN'), ('problem', 'NN'), ('Stac', 'NNP'), ('Technologies', 'NNPS'), ('owners', 'NNS'), ('board', 'NN'), ('compression', 'NN'), ('technology', 'NN'), ('I', 'PRP'), ('writing', 'VBG'), ('memory', 'NN'), ('I', 'PRP'), ('lost', 'VBD'), ('reference', 'NN'), ('Please', 'NNP'), ('correct', 'NN'), ('I', 'PRP'), ('wrong', 'VBP'), ('Using', 'VBG'), ('board', 'NN'), ('I', 'PRP'), ('problems', 'NNS'), ('file', 'VBP'), ('icons', 'NNS'), ('lost', 'VBN'), ('hard', 'RB'), ('say', 'VBP'), ('whether', 'IN'), ('board', 'NN'), ('fault', 'NN'), ('something', 'NN'), ('else', 'RB'), ('however', 'RB'), ('I', 'PRP'), ('decompress', 'VBP'), ('troubled', 'JJ'), ('file', 'NN'), ('recompress', 'NN'), ('without', 'IN'), ('board', 'NN'), ('icon', 'NN'), ('usually', 'RB'), ('reappears', 'VBZ'), ('Because', 'IN'), ('mentioned', 'VBN'), ('licensing', 'NN'), ('problem', 'NN'), ('freeware', 'NN'), ('expansion', 'NN'), ('utility', 'NN'), ('DD', 'NNP'), ('Expand', 'NNP'), ('decompress', 'NNP'), ('board', 'NN'), ('compressed', 'VBD'), ('file', 'NN'), ('unless', 'IN'), ('board', 'NN'), ('installed', 'VBN'), ('Since', 'IN'), ('Stac', 'NNP'), ('product', 'NN'), ('seems', 'VBZ'), ('unlikely', 'JJ'), ('holes', 'NNS'), ('Autodoubler', 'NNP'), ('Diskdoubler', 'NNP'), ('related', 'VBD'), ('board', 'NN'), ('fixed', 'VBN'), ('Which', 'NNP'), ('sad', 'JJ'), ('makes', 'VBZ'), ('reluctant', 'JJ'), ('buy', 'VB'), ('Stac', 'NNP'), ('product', 'NN'), ('since', 'IN'), ('stinky', 'NN'), ('But', 'CC'), ('hey', 'JJ'), ('competition', 'NN'), ('Stan', 'NNP'), ('Kerr', 'NNP'), ('Computing', 'NNP'), ('Communications', 'NNP'), ('Services', 'NNP'), ('Office', 'NNP'), ('U', 'NNP'), ('Illinois', 'NNP'), ('Urbana', 'NNP'), ('Phone', 'NNP'), ('217', 'CD'), ('333', 'CD'), ('5217', 'CD'), ('Email', 'NNP'), ('stankerr', 'NN'), ('uiuc', 'NN'), ('edu', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tag_filtered_text2 = nltk.pos_tag(filtered_text_var2)\n",
    "print tag_filtered_text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('From', 'IN'), ('twillis', 'JJ'), ('ec', 'NN'), ('ecn', 'NNS'), ('purdue', 'JJ'), ('edu', 'VBP'), ('Thomas', 'NNP'), ('E', 'NNP'), ('Willis', 'NNP'), ('Subject', 'NNP'), ('PB', 'NNP'), ('questions', 'NNS'), ('Organization', 'NNP'), ('Purdue', 'NNP'), ('University', 'NNP'), ('Engineering', 'NNP'), ('Computer', 'NNP'), ('Network', 'NNP'), ('Distribution', 'NNP'), ('usa', 'NNP'), ('Lines', 'VBZ'), ('36', 'CD'), ('well', 'RB'), ('folks', 'NNS'), ('mac', 'VBP'), ('plus', 'CC'), ('finally', 'RB'), ('gave', 'VBD'), ('ghost', 'VBN'), ('weekend', 'NN'), ('starting', 'VBG'), ('life', 'NN'), ('512k', 'CD'), ('way', 'NN'), ('back', 'RB'), ('1985', 'CD'), ('sooo', 'NN'), ('market', 'NN'), ('new', 'JJ'), ('machine', 'NN'), ('bit', 'NN'), ('sooner', 'RBR'), ('intended', 'VBD'), ('looking', 'VBG'), ('picking', 'VBG'), ('powerbook', 'NN'), ('160', 'CD'), ('maybe', 'RB'), ('180', 'CD'), ('bunch', 'JJ'), ('questions', 'NNS'), ('hopefully', 'RB'), ('somebody', 'VBP'), ('answer', 'JJR'), ('anybody', 'NN'), ('know', 'VBP'), ('dirt', 'NN'), ('next', 'JJ'), ('round', 'NN'), ('powerbook', 'NN'), ('introductions', 'NNS'), ('expected', 'VBD'), ('heard', 'RB'), ('185c', 'CD'), ('supposed', 'VBD'), ('make', 'NN'), ('appearence', 'NN'), ('summer', 'NN'), ('heard', 'NN'), ('anymore', 'NN'), ('since', 'IN'), ('access', 'NN'), ('macleak', 'NN'), ('wondering', 'VBG'), ('anybody', 'NN'), ('info', 'VB'), ('anybody', 'NN'), ('heard', 'JJ'), ('rumors', 'NNS'), ('price', 'NN'), ('drops', 'NNS'), ('powerbook', 'VBP'), ('line', 'NN'), ('like', 'IN'), ('ones', 'NNS'), ('duo', 'FW'), ('went', 'VBD'), ('recently', 'RB'), ('impression', 'NN'), ('display', 'NN'), ('180', 'CD'), ('could', 'MD'), ('probably', 'RB'), ('swing', 'VBG'), ('180', 'CD'), ('got', 'VBD'), ('80Mb', 'CD'), ('disk', 'NN'), ('rather', 'RB'), ('120', 'CD'), ('really', 'RB'), ('feel', 'VB'), ('much', 'RB'), ('better', 'JJR'), ('display', 'NN'), ('yea', 'NN'), ('looks', 'VBZ'), ('great', 'JJ'), ('store', 'NN'), ('wow', 'NN'), ('really', 'RB'), ('good', 'JJ'), ('could', 'MD'), ('solicit', 'VB'), ('opinions', 'NNS'), ('people', 'NNS'), ('use', 'VBP'), ('160', 'CD'), ('180', 'CD'), ('day', 'NN'), ('day', 'NN'), ('worth', 'IN'), ('taking', 'VBG'), ('disk', 'JJ'), ('size', 'NN'), ('money', 'NN'), ('hit', 'VBN'), ('get', 'VBP'), ('active', 'JJ'), ('display', 'NN'), ('realize', 'VB'), ('real', 'JJ'), ('subjective', 'JJ'), ('question', 'NN'), ('played', 'VBN'), ('around', 'IN'), ('machines', 'NNS'), ('computer', 'NN'), ('store', 'NN'), ('breifly', 'NN'), ('figured', 'VBD'), ('opinions', 'NNS'), ('somebody', 'NN'), ('actually', 'RB'), ('uses', 'VBZ'), ('machine', 'NN'), ('daily', 'JJ'), ('might', 'MD'), ('prove', 'VB'), ('helpful', 'JJ'), ('well', 'RB'), ('hellcats', 'NNS'), ('perform', 'VBP'), ('thanks', 'NNS'), ('bunch', 'JJ'), ('advance', 'NN'), ('info', 'NN'), ('could', 'MD'), ('email', 'VB'), ('post', 'NN'), ('summary', 'JJ'), ('news', 'NN'), ('reading', 'NN'), ('time', 'NN'), ('premium', 'JJ'), ('finals', 'NNS'), ('around', 'IN'), ('corner', 'NN'), ('Tom', 'NNP'), ('Willis', 'NNP'), ('twillis', 'NN'), ('ecn', 'NN'), ('purdue', 'JJ'), ('edu', 'NN'), ('Purdue', 'NNP'), ('Electrical', 'NNP'), ('Engineering', 'NNP'), ('Convictions', 'NNP'), ('dangerous', 'JJ'), ('enemies', 'NNS'), ('truth', 'NN'), ('lies', 'VBZ'), ('F', 'NNP'), ('W', 'NNP'), ('Nietzsche', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "tag_filtered_text3 = nltk.pos_tag(filtered_text_var3)\n",
    "print tag_filtered_text3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  From/IN\n",
      "  (NP lerxst/JJ wam/NN)\n",
      "  (NP\n",
      "    umd/JJ\n",
      "    edu/JJ\n",
      "    thing/NN\n",
      "    Subject/NNP\n",
      "    WHAT/NNP\n",
      "    car/NN\n",
      "    Nntp/NNP\n",
      "    Posting/NNP\n",
      "    Host/NNP\n",
      "    rac3/NN\n",
      "    wam/NN)\n",
      "  (NP\n",
      "    umd/JJ\n",
      "    edu/JJ\n",
      "    Organization/NNP\n",
      "    University/NNP\n",
      "    Maryland/NNP\n",
      "    College/NNP\n",
      "    Park/NNP\n",
      "    Lines/NNP)\n",
      "  15/CD\n",
      "  I/PRP\n",
      "  wondering/VBG\n",
      "  (NP anyone/NN)\n",
      "  could/MD\n",
      "  enlighten/VB\n",
      "  (NP car/NN)\n",
      "  I/PRP\n",
      "  saw/VBD\n",
      "  (NP day/NN)\n",
      "  It/PRP\n",
      "  2/CD\n",
      "  (NP door/JJ sports/NNS car/NN)\n",
      "  looked/VBD\n",
      "  late/RB\n",
      "  60s/CD\n",
      "  early/JJ\n",
      "  70s/CD\n",
      "  It/PRP\n",
      "  called/VBD\n",
      "  (NP Bricklin/NNP)\n",
      "  (NP The/DT doors/NNS)\n",
      "  really/RB\n",
      "  small/JJ\n",
      "  In/IN\n",
      "  (NP addition/NN)\n",
      "  (NP front/JJ bumper/NN)\n",
      "  (NP separate/JJ rest/NN body/NN)\n",
      "  This/DT\n",
      "  I/PRP\n",
      "  know/VBP\n",
      "  If/IN\n",
      "  (NP\n",
      "    anyone/NN\n",
      "    tellme/NN\n",
      "    model/NN\n",
      "    name/NN\n",
      "    engine/NN\n",
      "    specs/NN\n",
      "    years/NNS\n",
      "    production/NN\n",
      "    car/NN)\n",
      "  made/VBD\n",
      "  (NP history/NN)\n",
      "  whatever/WDT\n",
      "  (NP info/NN funky/NN)\n",
      "  looking/VBG\n",
      "  (NP car/NN please/NN)\n",
      "  (NP e/JJ mail/NN Thanks/NNP IL/NNP)\n",
      "  brought/VBD\n",
      "  (NP neighborhood/NN Lerxst/NN))\n",
      "(NP lerxst/JJ wam/NN)\n",
      "(NP\n",
      "  umd/JJ\n",
      "  edu/JJ\n",
      "  thing/NN\n",
      "  Subject/NNP\n",
      "  WHAT/NNP\n",
      "  car/NN\n",
      "  Nntp/NNP\n",
      "  Posting/NNP\n",
      "  Host/NNP\n",
      "  rac3/NN\n",
      "  wam/NN)\n",
      "(NP\n",
      "  umd/JJ\n",
      "  edu/JJ\n",
      "  Organization/NNP\n",
      "  University/NNP\n",
      "  Maryland/NNP\n",
      "  College/NNP\n",
      "  Park/NNP\n",
      "  Lines/NNP)\n",
      "(NP anyone/NN)\n",
      "(NP car/NN)\n",
      "(NP day/NN)\n",
      "(NP door/JJ sports/NNS car/NN)\n",
      "(NP Bricklin/NNP)\n",
      "(NP The/DT doors/NNS)\n",
      "(NP addition/NN)\n",
      "(NP front/JJ bumper/NN)\n",
      "(NP separate/JJ rest/NN body/NN)\n",
      "(NP\n",
      "  anyone/NN\n",
      "  tellme/NN\n",
      "  model/NN\n",
      "  name/NN\n",
      "  engine/NN\n",
      "  specs/NN\n",
      "  years/NNS\n",
      "  production/NN\n",
      "  car/NN)\n",
      "(NP history/NN)\n",
      "(NP info/NN funky/NN)\n",
      "(NP car/NN please/NN)\n",
      "(NP e/JJ mail/NN Thanks/NNP IL/NNP)\n",
      "(NP neighborhood/NN Lerxst/NN)\n"
     ]
    }
   ],
   "source": [
    "# matching noun phrases using the regular expression <DT>?<JJ.*>*<NN.*>+ as the grammar.\n",
    "grammar = \"NP: {<DT>?<JJ.*>*<NN.*>+}\"\n",
    "chunkParser  =nltk.RegexpParser(grammar)\n",
    "result = chunkParser.parse(tag_filtered_text1)\n",
    "for subtree in result.subtrees():\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  From/IN\n",
      "  kerr/VBN\n",
      "  (NP ux1/JJ cso/NN)\n",
      "  (NP\n",
      "    uiuc/JJ\n",
      "    edu/NN\n",
      "    Stan/NNP\n",
      "    Kerr/NNP\n",
      "    Subject/NNP\n",
      "    Re/NNP\n",
      "    Sigma/NNP\n",
      "    Designs/NNP\n",
      "    Double/NNP\n",
      "    Article/NNP)\n",
      "  I/PRP\n",
      "  (NP D/NNP)\n",
      "  (NP\n",
      "    ux1/JJ\n",
      "    C52u8x/NNP\n",
      "    B62/NNP\n",
      "    Organization/NNP\n",
      "    University/NNP\n",
      "    Illinois/NNP\n",
      "    Urbana/NNP\n",
      "    Lines/NNP)\n",
      "  29/CD\n",
      "  (NP\n",
      "    jap10/NN\n",
      "    po/NN\n",
      "    CWRU/NNP\n",
      "    Edu/NNP\n",
      "    Joseph/NNP\n",
      "    A/NNP\n",
      "    Pellettiere/NNP)\n",
      "  writes/VBZ\n",
      "  I/PRP\n",
      "  looking/VBG\n",
      "  (NP information/NN Sigma/NNP Designs/NNP)\n",
      "  double/RB\n",
      "  (NP board/NN)\n",
      "  All/DT\n",
      "  I/PRP\n",
      "  figure/VBP\n",
      "  (NP hardware/JJ compression/NN board/NN)\n",
      "  works/VBZ\n",
      "  (NP AutoDoubler/NNP)\n",
      "  I/PRP\n",
      "  sure/VBP\n",
      "  Also/RB\n",
      "  much/RB\n",
      "  would/MD\n",
      "  one/CD\n",
      "  (NP cost/NN)\n",
      "  I/PRP\n",
      "  (NP board/NN year/NN work/NN Diskdoubler/NNP Autodoubler/NNP)\n",
      "  (NP\n",
      "    due/JJ\n",
      "    licensing/NN\n",
      "    problem/NN\n",
      "    Stac/NNP\n",
      "    Technologies/NNPS\n",
      "    owners/NNS\n",
      "    board/NN\n",
      "    compression/NN\n",
      "    technology/NN)\n",
      "  I/PRP\n",
      "  writing/VBG\n",
      "  (NP memory/NN)\n",
      "  I/PRP\n",
      "  lost/VBD\n",
      "  (NP reference/NN Please/NNP correct/NN)\n",
      "  I/PRP\n",
      "  wrong/VBP\n",
      "  Using/VBG\n",
      "  (NP board/NN)\n",
      "  I/PRP\n",
      "  (NP problems/NNS)\n",
      "  file/VBP\n",
      "  (NP icons/NNS)\n",
      "  lost/VBN\n",
      "  hard/RB\n",
      "  say/VBP\n",
      "  whether/IN\n",
      "  (NP board/NN fault/NN something/NN)\n",
      "  else/RB\n",
      "  however/RB\n",
      "  I/PRP\n",
      "  decompress/VBP\n",
      "  (NP troubled/JJ file/NN recompress/NN)\n",
      "  without/IN\n",
      "  (NP board/NN icon/NN)\n",
      "  usually/RB\n",
      "  reappears/VBZ\n",
      "  Because/IN\n",
      "  mentioned/VBN\n",
      "  (NP\n",
      "    licensing/NN\n",
      "    problem/NN\n",
      "    freeware/NN\n",
      "    expansion/NN\n",
      "    utility/NN\n",
      "    DD/NNP\n",
      "    Expand/NNP\n",
      "    decompress/NNP\n",
      "    board/NN)\n",
      "  compressed/VBD\n",
      "  (NP file/NN)\n",
      "  unless/IN\n",
      "  (NP board/NN)\n",
      "  installed/VBN\n",
      "  Since/IN\n",
      "  (NP Stac/NNP product/NN)\n",
      "  seems/VBZ\n",
      "  (NP unlikely/JJ holes/NNS Autodoubler/NNP Diskdoubler/NNP)\n",
      "  related/VBD\n",
      "  (NP board/NN)\n",
      "  fixed/VBN\n",
      "  (NP Which/NNP)\n",
      "  sad/JJ\n",
      "  makes/VBZ\n",
      "  reluctant/JJ\n",
      "  buy/VB\n",
      "  (NP Stac/NNP product/NN)\n",
      "  since/IN\n",
      "  (NP stinky/NN)\n",
      "  But/CC\n",
      "  (NP\n",
      "    hey/JJ\n",
      "    competition/NN\n",
      "    Stan/NNP\n",
      "    Kerr/NNP\n",
      "    Computing/NNP\n",
      "    Communications/NNP\n",
      "    Services/NNP\n",
      "    Office/NNP\n",
      "    U/NNP\n",
      "    Illinois/NNP\n",
      "    Urbana/NNP\n",
      "    Phone/NNP)\n",
      "  217/CD\n",
      "  333/CD\n",
      "  5217/CD\n",
      "  (NP Email/NNP stankerr/NN uiuc/NN edu/NN))\n",
      "(NP ux1/JJ cso/NN)\n",
      "(NP\n",
      "  uiuc/JJ\n",
      "  edu/NN\n",
      "  Stan/NNP\n",
      "  Kerr/NNP\n",
      "  Subject/NNP\n",
      "  Re/NNP\n",
      "  Sigma/NNP\n",
      "  Designs/NNP\n",
      "  Double/NNP\n",
      "  Article/NNP)\n",
      "(NP D/NNP)\n",
      "(NP\n",
      "  ux1/JJ\n",
      "  C52u8x/NNP\n",
      "  B62/NNP\n",
      "  Organization/NNP\n",
      "  University/NNP\n",
      "  Illinois/NNP\n",
      "  Urbana/NNP\n",
      "  Lines/NNP)\n",
      "(NP jap10/NN po/NN CWRU/NNP Edu/NNP Joseph/NNP A/NNP Pellettiere/NNP)\n",
      "(NP information/NN Sigma/NNP Designs/NNP)\n",
      "(NP board/NN)\n",
      "(NP hardware/JJ compression/NN board/NN)\n",
      "(NP AutoDoubler/NNP)\n",
      "(NP cost/NN)\n",
      "(NP board/NN year/NN work/NN Diskdoubler/NNP Autodoubler/NNP)\n",
      "(NP\n",
      "  due/JJ\n",
      "  licensing/NN\n",
      "  problem/NN\n",
      "  Stac/NNP\n",
      "  Technologies/NNPS\n",
      "  owners/NNS\n",
      "  board/NN\n",
      "  compression/NN\n",
      "  technology/NN)\n",
      "(NP memory/NN)\n",
      "(NP reference/NN Please/NNP correct/NN)\n",
      "(NP board/NN)\n",
      "(NP problems/NNS)\n",
      "(NP icons/NNS)\n",
      "(NP board/NN fault/NN something/NN)\n",
      "(NP troubled/JJ file/NN recompress/NN)\n",
      "(NP board/NN icon/NN)\n",
      "(NP\n",
      "  licensing/NN\n",
      "  problem/NN\n",
      "  freeware/NN\n",
      "  expansion/NN\n",
      "  utility/NN\n",
      "  DD/NNP\n",
      "  Expand/NNP\n",
      "  decompress/NNP\n",
      "  board/NN)\n",
      "(NP file/NN)\n",
      "(NP board/NN)\n",
      "(NP Stac/NNP product/NN)\n",
      "(NP unlikely/JJ holes/NNS Autodoubler/NNP Diskdoubler/NNP)\n",
      "(NP board/NN)\n",
      "(NP Which/NNP)\n",
      "(NP Stac/NNP product/NN)\n",
      "(NP stinky/NN)\n",
      "(NP\n",
      "  hey/JJ\n",
      "  competition/NN\n",
      "  Stan/NNP\n",
      "  Kerr/NNP\n",
      "  Computing/NNP\n",
      "  Communications/NNP\n",
      "  Services/NNP\n",
      "  Office/NNP\n",
      "  U/NNP\n",
      "  Illinois/NNP\n",
      "  Urbana/NNP\n",
      "  Phone/NNP)\n",
      "(NP Email/NNP stankerr/NN uiuc/NN edu/NN)\n"
     ]
    }
   ],
   "source": [
    "# matching noun phrases using the regular expression <DT>?<JJ.*>*<NN.*>+ as the grammar.\n",
    "grammar = \"NP: {<DT>?<JJ.*>*<NN.*>+}\"\n",
    "chunkParser  =nltk.RegexpParser(grammar)\n",
    "result = chunkParser.parse(tag_filtered_text2)\n",
    "for subtree in result.subtrees():\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  From/IN\n",
      "  (NP twillis/JJ ec/NN ecn/NNS)\n",
      "  purdue/JJ\n",
      "  edu/VBP\n",
      "  (NP\n",
      "    Thomas/NNP\n",
      "    E/NNP\n",
      "    Willis/NNP\n",
      "    Subject/NNP\n",
      "    PB/NNP\n",
      "    questions/NNS\n",
      "    Organization/NNP\n",
      "    Purdue/NNP\n",
      "    University/NNP\n",
      "    Engineering/NNP\n",
      "    Computer/NNP\n",
      "    Network/NNP\n",
      "    Distribution/NNP\n",
      "    usa/NNP)\n",
      "  Lines/VBZ\n",
      "  36/CD\n",
      "  well/RB\n",
      "  (NP folks/NNS)\n",
      "  mac/VBP\n",
      "  plus/CC\n",
      "  finally/RB\n",
      "  gave/VBD\n",
      "  ghost/VBN\n",
      "  (NP weekend/NN)\n",
      "  starting/VBG\n",
      "  (NP life/NN)\n",
      "  512k/CD\n",
      "  (NP way/NN)\n",
      "  back/RB\n",
      "  1985/CD\n",
      "  (NP sooo/NN market/NN)\n",
      "  (NP new/JJ machine/NN bit/NN)\n",
      "  sooner/RBR\n",
      "  intended/VBD\n",
      "  looking/VBG\n",
      "  picking/VBG\n",
      "  (NP powerbook/NN)\n",
      "  160/CD\n",
      "  maybe/RB\n",
      "  180/CD\n",
      "  (NP bunch/JJ questions/NNS)\n",
      "  hopefully/RB\n",
      "  somebody/VBP\n",
      "  (NP answer/JJR anybody/NN)\n",
      "  know/VBP\n",
      "  (NP dirt/NN)\n",
      "  (NP next/JJ round/NN powerbook/NN introductions/NNS)\n",
      "  expected/VBD\n",
      "  heard/RB\n",
      "  185c/CD\n",
      "  supposed/VBD\n",
      "  (NP make/NN appearence/NN summer/NN heard/NN anymore/NN)\n",
      "  since/IN\n",
      "  (NP access/NN macleak/NN)\n",
      "  wondering/VBG\n",
      "  (NP anybody/NN)\n",
      "  info/VB\n",
      "  (NP anybody/NN)\n",
      "  (NP heard/JJ rumors/NNS price/NN drops/NNS)\n",
      "  powerbook/VBP\n",
      "  (NP line/NN)\n",
      "  like/IN\n",
      "  (NP ones/NNS)\n",
      "  duo/FW\n",
      "  went/VBD\n",
      "  recently/RB\n",
      "  (NP impression/NN display/NN)\n",
      "  180/CD\n",
      "  could/MD\n",
      "  probably/RB\n",
      "  swing/VBG\n",
      "  180/CD\n",
      "  got/VBD\n",
      "  80Mb/CD\n",
      "  (NP disk/NN)\n",
      "  rather/RB\n",
      "  120/CD\n",
      "  really/RB\n",
      "  feel/VB\n",
      "  much/RB\n",
      "  (NP better/JJR display/NN yea/NN)\n",
      "  looks/VBZ\n",
      "  (NP great/JJ store/NN wow/NN)\n",
      "  really/RB\n",
      "  good/JJ\n",
      "  could/MD\n",
      "  solicit/VB\n",
      "  (NP opinions/NNS people/NNS)\n",
      "  use/VBP\n",
      "  160/CD\n",
      "  180/CD\n",
      "  (NP day/NN day/NN)\n",
      "  worth/IN\n",
      "  taking/VBG\n",
      "  (NP disk/JJ size/NN money/NN)\n",
      "  hit/VBN\n",
      "  get/VBP\n",
      "  (NP active/JJ display/NN)\n",
      "  realize/VB\n",
      "  (NP real/JJ subjective/JJ question/NN)\n",
      "  played/VBN\n",
      "  around/IN\n",
      "  (NP machines/NNS computer/NN store/NN breifly/NN)\n",
      "  figured/VBD\n",
      "  (NP opinions/NNS somebody/NN)\n",
      "  actually/RB\n",
      "  uses/VBZ\n",
      "  (NP machine/NN)\n",
      "  daily/JJ\n",
      "  might/MD\n",
      "  prove/VB\n",
      "  helpful/JJ\n",
      "  well/RB\n",
      "  (NP hellcats/NNS)\n",
      "  perform/VBP\n",
      "  (NP thanks/NNS)\n",
      "  (NP bunch/JJ advance/NN info/NN)\n",
      "  could/MD\n",
      "  email/VB\n",
      "  (NP post/NN)\n",
      "  (NP summary/JJ news/NN reading/NN time/NN)\n",
      "  (NP premium/JJ finals/NNS)\n",
      "  around/IN\n",
      "  (NP corner/NN Tom/NNP Willis/NNP twillis/NN ecn/NN)\n",
      "  (NP\n",
      "    purdue/JJ\n",
      "    edu/NN\n",
      "    Purdue/NNP\n",
      "    Electrical/NNP\n",
      "    Engineering/NNP\n",
      "    Convictions/NNP)\n",
      "  (NP dangerous/JJ enemies/NNS truth/NN)\n",
      "  lies/VBZ\n",
      "  (NP F/NNP W/NNP Nietzsche/NNP))\n",
      "(NP twillis/JJ ec/NN ecn/NNS)\n",
      "(NP\n",
      "  Thomas/NNP\n",
      "  E/NNP\n",
      "  Willis/NNP\n",
      "  Subject/NNP\n",
      "  PB/NNP\n",
      "  questions/NNS\n",
      "  Organization/NNP\n",
      "  Purdue/NNP\n",
      "  University/NNP\n",
      "  Engineering/NNP\n",
      "  Computer/NNP\n",
      "  Network/NNP\n",
      "  Distribution/NNP\n",
      "  usa/NNP)\n",
      "(NP folks/NNS)\n",
      "(NP weekend/NN)\n",
      "(NP life/NN)\n",
      "(NP way/NN)\n",
      "(NP sooo/NN market/NN)\n",
      "(NP new/JJ machine/NN bit/NN)\n",
      "(NP powerbook/NN)\n",
      "(NP bunch/JJ questions/NNS)\n",
      "(NP answer/JJR anybody/NN)\n",
      "(NP dirt/NN)\n",
      "(NP next/JJ round/NN powerbook/NN introductions/NNS)\n",
      "(NP make/NN appearence/NN summer/NN heard/NN anymore/NN)\n",
      "(NP access/NN macleak/NN)\n",
      "(NP anybody/NN)\n",
      "(NP anybody/NN)\n",
      "(NP heard/JJ rumors/NNS price/NN drops/NNS)\n",
      "(NP line/NN)\n",
      "(NP ones/NNS)\n",
      "(NP impression/NN display/NN)\n",
      "(NP disk/NN)\n",
      "(NP better/JJR display/NN yea/NN)\n",
      "(NP great/JJ store/NN wow/NN)\n",
      "(NP opinions/NNS people/NNS)\n",
      "(NP day/NN day/NN)\n",
      "(NP disk/JJ size/NN money/NN)\n",
      "(NP active/JJ display/NN)\n",
      "(NP real/JJ subjective/JJ question/NN)\n",
      "(NP machines/NNS computer/NN store/NN breifly/NN)\n",
      "(NP opinions/NNS somebody/NN)\n",
      "(NP machine/NN)\n",
      "(NP hellcats/NNS)\n",
      "(NP thanks/NNS)\n",
      "(NP bunch/JJ advance/NN info/NN)\n",
      "(NP post/NN)\n",
      "(NP summary/JJ news/NN reading/NN time/NN)\n",
      "(NP premium/JJ finals/NNS)\n",
      "(NP corner/NN Tom/NNP Willis/NNP twillis/NN ecn/NN)\n",
      "(NP\n",
      "  purdue/JJ\n",
      "  edu/NN\n",
      "  Purdue/NNP\n",
      "  Electrical/NNP\n",
      "  Engineering/NNP\n",
      "  Convictions/NNP)\n",
      "(NP dangerous/JJ enemies/NNS truth/NN)\n",
      "(NP F/NNP W/NNP Nietzsche/NNP)\n"
     ]
    }
   ],
   "source": [
    "# matching noun phrases using the regular expression <DT>?<JJ.*>*<NN.*>+ as the grammar.\n",
    "grammar = \"NP: {<DT>?<JJ.*>*<NN.*>+}\"\n",
    "chunkParser  =nltk.RegexpParser(grammar)\n",
    "result = chunkParser.parse(tag_filtered_text3)\n",
    "for subtree in result.subtrees():\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning text\n",
    "from nltk.tag import untag \n",
    "cleaned_text1 = untag(tag_filtered_text1) \n",
    "\n",
    "cleaned_text2 = untag(tag_filtered_text2) \n",
    "\n",
    "cleaned_text3 = untag(tag_filtered_text3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted in the TfidfVectorizer function\n",
    "tfv = TfidfVectorizer(stop_words = stop_words, ngram_range = (1,3))\n",
    "vec_text1 = tfv.fit_transform(cleaned_text1)\n",
    "words = tfv.get_feature_names()\n",
    "vec_text2 = tfv.fit_transform(cleaned_text2)\n",
    "words2 = tfv.get_feature_names()\n",
    "vec_text3 = tfv.fit_transform(cleaned_text3)\n",
    "words3 = tfv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 1, Category 0 (after 6 stepText-Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : wam, years, door, late, know, info, il, host, history, funky\n",
      "1 : bumper, years, doors, lerxst, late, know, info, il, host, history\n",
      "2 : saw, years, door, late, know, info, il, host, history, funky\n",
      "3 : years, model, looking, lines, late, know, info, host, history, funky\n",
      "4 : umd, years, door, late, know, info, il, host, history, funky\n",
      "5 : mail, years, doors, late, know, info, il, host, history, funky\n",
      "6 : anyone, years, doors, lerxst, late, know, info, il, host, history\n",
      "7 : edu, years, doors, lerxst, late, know, info, il, host, history\n",
      "8 : rac3, years, doors, late, know, info, il, host, history, funky\n",
      "9 : college, years, doors, lerxst, late, know, info, il, host, history\n",
      "10 : small, years, door, late, know, info, il, host, history, funky\n",
      "11 : maryland, years, doors, late, know, info, il, host, history, funky\n",
      "12 : nntp, years, doors, late, know, info, il, host, history, funky\n",
      "13 : tellme, years, door, late, know, info, il, host, history, funky\n",
      "14 : il, years, doors, lerxst, late, know, info, host, history, funky\n",
      "15 : posting, years, doors, late, know, info, il, host, history, funky\n",
      "16 : car, years, doors, lerxst, late, know, info, il, host, history\n",
      "17 : specs, years, door, late, know, info, il, host, history, funky\n",
      "18 : lerxst, years, doors, late, know, info, il, host, history, funky\n",
      "19 : wondering, years, doors, lerxst, late, know, info, il, host, history\n"
     ]
    }
   ],
   "source": [
    "#setup kmeans clustering\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = 20, n_init = 17, n_jobs = -1, tol = 0.01, max_iter = 200)#fit the data \n",
    "kmeans.fit(vec_text1)#this loop transforms the numbers back into words\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 2, Category 9 (after 6 stepText-Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : product, year, hey, email, expand, expansion, fault, figure, file, fixed\n",
      "1 : autodoubler, year, holes, email, expand, expansion, fault, figure, file, fixed\n",
      "2 : diskdoubler, year, hey, email, expand, expansion, fault, figure, file, fixed\n",
      "3 : uiuc, year, hey, email, expand, expansion, fault, figure, file, fixed\n",
      "4 : since, designs, licensing, stan, compression, year, holes, fault, figure, fixed\n",
      "5 : board, year, holes, email, expand, expansion, fault, figure, file, fixed\n",
      "6 : decompress, year, hey, email, expand, expansion, fault, figure, file, fixed\n",
      "7 : edu, year, email, expand, expansion, fault, figure, file, fixed, freeware\n",
      "8 : stac, hey, else, email, expand, expansion, fault, figure, file, fixed\n",
      "9 : illinois, year, hey, email, expand, expansion, fault, figure, file, fixed\n",
      "10 : jap10, year, hey, email, expand, expansion, fault, figure, file, fixed\n",
      "11 : sigma, year, hey, email, expand, expansion, fault, figure, file, fixed\n",
      "12 : double, year, hey, email, expand, expansion, fault, figure, file, fixed\n",
      "13 : urbana, holes, email, expand, expansion, fault, figure, file, fixed, freeware\n",
      "14 : lost, hey, else, email, expand, expansion, fault, figure, file, fixed\n",
      "15 : file, year, edu, email, expand, expansion, fault, figure, fixed, freeware\n",
      "16 : problem, holes, email, expand, expansion, fault, figure, file, fixed, freeware\n",
      "17 : kerr, year, hey, email, expand, expansion, fault, figure, file, fixed\n",
      "18 : wrong, holes, email, expand, expansion, fault, figure, file, fixed, freeware\n",
      "19 : ux1, year, holes, email, expand, expansion, fault, figure, file, fixed\n"
     ]
    }
   ],
   "source": [
    "kmeans.fit(vec_text2)#this loop transforms the numbers back into words\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words2[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 3, Category 2 (after 6 stepText-Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 512k, yea, finally, got, good, ghost, get, gave, folks, finals\n",
      "1 : heard, ecn, got, good, ghost, get, gave, folks, finals, finally\n",
      "2 : display, 160, really, engineering, well, disk, info, edu, store, opinions\n",
      "3 : could, yea, great, good, ghost, get, gave, folks, finals, finally\n",
      "4 : machine, ecn, got, good, ghost, get, gave, folks, finals, finally\n",
      "5 : 180, yea, finally, got, good, ghost, get, gave, folks, finals\n",
      "6 : willis, yea, figured, good, ghost, get, gave, folks, finals, finally\n",
      "7 : thomas, yea, finally, good, ghost, get, gave, folks, finals, figured\n",
      "8 : bunch, yea, great, good, ghost, get, gave, folks, finals, finally\n",
      "9 : somebody, yea, figured, ghost, get, gave, folks, finals, finally, feel\n",
      "10 : purdue, yea, finally, good, ghost, get, gave, folks, finals, figured\n",
      "11 : anybody, yea, finals, got, good, ghost, get, gave, folks, finally\n",
      "12 : like, yea, finally, good, ghost, get, gave, folks, finals, figured\n",
      "13 : day, yea, finally, good, ghost, get, gave, folks, finals, figured\n",
      "14 : questions, yea, finally, good, ghost, get, gave, folks, finals, figured\n",
      "15 : computer, yea, great, good, ghost, get, gave, folks, finals, finally\n",
      "16 : powerbook, yea, ecn, good, ghost, get, gave, folks, finals, finally\n",
      "17 : around, yea, finals, got, good, ghost, get, gave, folks, finally\n",
      "18 : ecn, yea, got, good, ghost, get, gave, folks, finals, finally\n",
      "19 : premium, yea, ecn, good, ghost, get, gave, folks, finals, finally\n"
     ]
    }
   ],
   "source": [
    "kmeans.fit(vec_text3)#this loop transforms the numbers back into words\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words3[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 4, Category 0 (same category as file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : cantaloupe, organization, apr, autos, back, barking, blip, bmw, chevy, clutch\n",
      "1 : driven, organization, com, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "2 : also, organization, com, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "3 : apr, organization, autos, back, barking, blip, bmw, cantaloupe, chevy, clutch\n",
      "4 : date, heard, fords, agian, gatech, id, message, current, chevy, clutch\n",
      "5 : anybody, organization, com, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "6 : 1993apr16, organization, comets, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "7 : 185329, organization, comets, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "8 : higher, com, art, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "9 : drove, organization, com, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "10 : lightly, organization, com, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "11 : 34, organization, autos, back, barking, blip, bmw, cantaloupe, chevy, clutch\n",
      "12 : done, organization, com, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "13 : cs, organization, com, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "14 : easier, com, art, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "15 : harder, organization, com, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "16 : 17, organization, comets, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "17 : 3505, organization, com, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "18 : morin, organization, comets, autos, back, barking, blip, bmw, cantaloupe, chevy\n",
      "19 : accelerator, organization, com, autos, back, barking, blip, bmw, cantaloupe, chevy\n"
     ]
    }
   ],
   "source": [
    "with open('/home/sreeganesh/ML LAB/ASSIGNMENT 6/20_newsgroups/rec.autos/102796c.txt', 'r') as myfile:\n",
    "  text4 = myfile.read()\n",
    "\n",
    "word_sent_token_text4 = (tokenizer.tokenize(text4))\n",
    "\n",
    "lemmatize_word_sent_token_text4 = []\n",
    "for i in range(0, len(word_sent_token_text4)):\n",
    "     l1 = word_sent_token_text4[i]\n",
    "     l2 = ''.join([lemmatizer.lemmatize(word) for word in l1])\n",
    "     lemmatize_word_sent_token_text4.append(l2)\n",
    "\n",
    "filtered_text_var4 = [str(r) for r in lemmatize_word_sent_token_text4]  \n",
    "\n",
    "tfv = TfidfVectorizer(stop_words = stop_words, ngram_range = (1,3))\n",
    "vec_text4 = tfv.fit_transform(filtered_text_var4)\n",
    "words = tfv.get_feature_names()\n",
    "\n",
    "kmeans.fit(vec_text2)#this loop transforms the numbers back into words\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 5, Category 9 (same category as file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : multiple, macs, devices, ac, 856, references, com, article, asked, believe\n",
      "1 : hamilton, cmu, anybody, apple, apr, article, asked, behaviour, believe, boards\n",
      "2 : c650, srv, addresses, anybody, apple, apr, article, asked, behaviour, believe\n",
      "3 : 16, srv, cmu, anybody, apple, apr, article, asked, behaviour, believe\n",
      "4 : 838, srv, clashing, anybody, apple, apr, article, asked, behaviour, believe\n",
      "5 : date, srv, clashing, anybody, apple, apr, article, asked, behaviour, believe\n",
      "6 : dangerous, srv, clashing, anybody, apple, apr, article, asked, behaviour, believe\n",
      "7 : one, srv, clashing, anybody, apple, apr, article, asked, behaviour, believe\n",
      "8 : mac, srv, clashing, anybody, apple, apr, article, asked, behaviour, believe\n",
      "9 : addresses, srv, anybody, apple, apr, article, asked, behaviour, believe, boards\n",
      "10 : 93, srv, clashing, anybody, apple, apr, article, asked, behaviour, believe\n",
      "11 : 15490, srv, cmu, anybody, apple, apr, article, asked, behaviour, believe\n",
      "12 : ldo, srv, clashing, anybody, apple, apr, article, asked, behaviour, believe\n",
      "13 : behaviour, srv, addresses, anybody, apple, apr, article, asked, believe, boards\n",
      "14 : couple, srv, clashing, anybody, apple, apr, article, asked, behaviour, believe\n",
      "15 : 20, srv, clashing, anybody, apple, apr, article, asked, behaviour, believe\n",
      "16 : message, clashing, ans, anybody, apple, apr, article, asked, behaviour, believe\n",
      "17 : heard, srv, clashing, anybody, apple, apr, article, asked, behaviour, believe\n",
      "18 : europa, srv, cmu, anybody, apple, apr, article, asked, behaviour, believe\n",
      "19 : people, cmu, anybody, apple, apr, article, asked, behaviour, believe, boards\n"
     ]
    }
   ],
   "source": [
    "with open('/home/sreeganesh/ML LAB/ASSIGNMENT 6/20_newsgroups/comp.sys.mac.hardware/51532.txt', 'r') as myfile:\n",
    "  text4 = myfile.read()\n",
    "\n",
    "word_sent_token_text4 = (tokenizer.tokenize(text4))\n",
    "\n",
    "lemmatize_word_sent_token_text4 = []\n",
    "for i in range(0, len(word_sent_token_text4)):\n",
    "     l1 = word_sent_token_text4[i]\n",
    "     l2 = ''.join([lemmatizer.lemmatize(word) for word in l1])\n",
    "     lemmatize_word_sent_token_text4.append(l2)\n",
    "\n",
    "filtered_text_var4 = [str(r) for r in lemmatize_word_sent_token_text4]  \n",
    "\n",
    "tfv = TfidfVectorizer(stop_words = stop_words, ngram_range = (1,3))\n",
    "vec_text4 = tfv.fit_transform(filtered_text_var4)\n",
    "words = tfv.get_feature_names()\n",
    "\n",
    "kmeans.fit(vec_text2)#this loop transforms the numbers back into words\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 6, Category 2 (same category as file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : date, sys, blow, care, cmu, com, comp, crabapple, cs, curious\n",
      "1 : price, div, care, cmu, com, comp, crabapple, cs, curious, date\n",
      "2 : electronics, know, around, area, apr, paid, mind, good, fs7, crabapple\n",
      "3 : midi, sys, difference, care, cmu, com, comp, crabapple, cs, curious\n",
      "4 : allows, sys, difference, care, cmu, com, comp, crabapple, cs, curious\n",
      "5 : money, difference, cantaloupe, care, cmu, com, comp, crabapple, cs, curious\n",
      "6 : 330, sys, div, care, cmu, com, comp, crabapple, cs, curious\n",
      "7 : fixed, sys, difference, care, cmu, com, comp, crabapple, cs, curious\n",
      "8 : adamsj, sys, difference, care, cmu, com, comp, crabapple, cs, curious\n",
      "9 : cs, sys, blow, care, cmu, com, comp, crabapple, curious, date\n",
      "10 : bay, sys, difference, care, cmu, com, comp, crabapple, cs, curious\n",
      "11 : blow, sys, care, cmu, com, comp, crabapple, cs, curious, date\n",
      "12 : mount, sys, difference, care, cmu, com, comp, crabapple, cs, curious\n",
      "13 : internal, div, care, cmu, com, comp, crabapple, cs, curious, date\n",
      "14 : drive, sys, difference, care, cmu, com, comp, crabapple, cs, curious\n",
      "15 : francisco, sys, difference, care, cmu, com, comp, crabapple, cs, curious\n",
      "16 : 27, sys, div, care, cmu, com, comp, crabapple, cs, curious\n",
      "17 : depend, sys, blow, care, cmu, com, comp, crabapple, cs, curious\n",
      "18 : reston, sys, div, care, cmu, com, comp, crabapple, cs, curious\n",
      "19 : product, sys, div, care, cmu, com, comp, crabapple, cs, curious\n"
     ]
    }
   ],
   "source": [
    "with open('/home/sreeganesh/ML LAB/ASSIGNMENT 6/20_newsgroups/comp.sys.mac.hardware/51578.txt', 'r') as myfile:\n",
    "  text4 = myfile.read()\n",
    "\n",
    "word_sent_token_text4 = (tokenizer.tokenize(text4))\n",
    "\n",
    "lemmatize_word_sent_token_text4 = []\n",
    "for i in range(0, len(word_sent_token_text4)):\n",
    "     l1 = word_sent_token_text4[i]\n",
    "     l2 = ''.join([lemmatizer.lemmatize(word) for word in l1])\n",
    "     lemmatize_word_sent_token_text4.append(l2)\n",
    "\n",
    "filtered_text_var4 = [str(r) for r in lemmatize_word_sent_token_text4]  \n",
    "\n",
    "tfv = TfidfVectorizer(stop_words = stop_words, ngram_range = (1,3))\n",
    "vec_text4 = tfv.fit_transform(filtered_text_var4)\n",
    "words = tfv.get_feature_names()\n",
    "\n",
    "kmeans.fit(vec_text2)#this loop transforms the numbers back into words\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
